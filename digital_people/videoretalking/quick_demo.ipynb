{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaohuiwu/SimpleFS/blob/master/digital_people/videoretalking/quick_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VideoReTalking：Audio-based Lip Synchronization for Talking Head Video Editing In the Wild\n",
        "\n",
        "[Arxiv](https://arxiv.org/abs/2211.14758) | [Project](https://vinthony.github.io/video-retalking/) | [Github](https://github.com/vinthony/video-retalking)\n",
        "\n",
        "Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, Nannan Wang\n",
        "\n",
        "Xidian University, Tencent AI Lab, Tsinghua University\n",
        "\n",
        "*SIGGRAPH Asia 2022 Conferenence Track*\n",
        "\n"
      ],
      "metadata": {
        "id": "NVfkv2BXSpr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installation** (30s)"
      ],
      "metadata": {
        "id": "u9hdPaH6UL_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PnKT9goiQ3Hk",
        "outputId": "f1f3581b-e542-4116-d826-02921b192d05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 26 00:17:15 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Python 3.12.11\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Git clone project and install requirements...\n",
            "/content/video-retalking/video-retalking\n",
            "Collecting basicsr==1.4.2 (from -r requirements.txt (line 1))\n",
            "  Using cached basicsr-1.4.2.tar.gz (172 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kornia==0.5.1 (from -r requirements.txt (line 2))\n",
            "  Using cached kornia-0.5.1-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting face-alignment==1.3.4 (from -r requirements.txt (line 3))\n",
            "  Using cached face_alignment-1.3.4.tar.gz (26 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ninja==1.10.2.3 (from -r requirements.txt (line 4))\n",
            "  Using cached ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting einops==0.4.1 (from -r requirements.txt (line 5))\n",
            "  Using cached einops-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting facexlib==0.2.5 (from -r requirements.txt (line 6))\n",
            "  Using cached facexlib-0.2.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 7))\n",
            "  Using cached librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting dlib==19.24.0 (from -r requirements.txt (line 8))\n",
            "  Using cached dlib-19.24.0.tar.gz (3.2 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gradio>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (5.42.0)\n",
            "Collecting numpy==1.23.4 (from -r requirements.txt (line 10))\n",
            "  Using cached numpy-1.23.4.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "### make sure that CUDA is available in Edit -> Nootbook settings -> GPU\n",
        "!nvidia-smi\n",
        "\n",
        "!python --version\n",
        "!apt-get update\n",
        "!apt install ffmpeg &> /dev/null\n",
        "\n",
        "print('Git clone project and install requirements...')\n",
        "!git clone https://github.com/vinthony/video-retalking.git &> /dev/null\n",
        "%cd video-retalking\n",
        "# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -r requirements.txt --use-pep517"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Pretrained Models**"
      ],
      "metadata": {
        "id": "uwJS0eaM61Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "print('Download pre-trained models...')\n",
        "!mkdir ./checkpoints\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/30_net_gen.pth -O ./checkpoints/30_net_gen.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/BFM.zip -O ./checkpoints/BFM.zip\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/DNet.pt -O ./checkpoints/DNet.pt\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ENet.pth -O ./checkpoints/ENet.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/expression.mat -O ./checkpoints/expression.mat\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/face3d_pretrain_epoch_20.pth -O ./checkpoints/face3d_pretrain_epoch_20.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GFPGANv1.3.pth -O ./checkpoints/GFPGANv1.3.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GPEN-BFR-512.pth -O ./checkpoints/GPEN-BFR-512.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/LNet.pth -O ./checkpoints/LNet.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ParseNet-latest.pth -O ./checkpoints/ParseNet-latest.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/RetinaFace-R50.pth -O ./checkpoints/RetinaFace-R50.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/shape_predictor_68_face_landmarks.dat -O ./checkpoints/shape_predictor_68_face_landmarks.dat\n",
        "!unzip -d ./checkpoints/BFM ./checkpoints/BFM.zip"
      ],
      "metadata": {
        "id": "x18qYuQY678E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**\n",
        "\n",
        "`--face`: Input video.\n",
        "\n",
        "`--audio`: Input audio. Both *.wav* and *.mp4* files are supported.\n",
        "\n",
        "You can choose our provided data from `./examples` folder or upload from your local computer.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QJRTF4U8UOjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import glob, os, sys\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "print(\"Choose the Video name to edit: (saved in folder 'examples/face')\")\n",
        "vid_list = glob.glob1('examples/face/', '*.mp4')\n",
        "vid_list.sort()\n",
        "default_vid_name = widgets.Dropdown(options=vid_list, value='1.mp4')\n",
        "display(default_vid_name)\n",
        "\n",
        "print(\"Choose the Audio name to edit: (saved in folder 'examples/audio')\")\n",
        "audio_list = glob.glob1('examples/audio/', '*')\n",
        "audio_list.sort()\n",
        "default_audio_name = widgets.Dropdown(options=audio_list, value='1.wav')\n",
        "display(default_audio_name)\n"
      ],
      "metadata": {
        "id": "U-IY-cBSporP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the input video and audio:"
      ],
      "metadata": {
        "id": "-MtI_R1bLJ-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "input_video_name = './examples/face/{}'.format(default_vid_name.value)\n",
        "input_video_mp4 = open('{}'.format(input_video_name),'rb').read()\n",
        "input_video_data_url = \"data:video/x-m4v;base64,\" + b64encode(input_video_mp4).decode()\n",
        "print('Display input video: {}'.format(input_video_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % input_video_data_url))\n",
        "\n",
        "input_audio_name = './examples/audio/{}'.format(default_audio_name.value)\n",
        "input_audio_mp4 = open('{}'.format(input_audio_name),'rb').read()\n",
        "input_audio_data_url = \"data:audio/wav;base64,\" + b64encode(input_audio_mp4).decode()\n",
        "print('Display input audio: {}'.format(input_audio_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <audio width=400 controls>\n",
        "        <source src=\"%s\" type=\"audio/wav\">\n",
        "  </audio>\n",
        "  \"\"\" % input_audio_data_url))\n"
      ],
      "metadata": {
        "id": "ljbScdofJyGO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_video_path = 'examples/face/{}'.format(default_vid_name.value)\n",
        "input_audio_path = 'examples/audio/{}'.format(default_audio_name.value)\n",
        "\n",
        "!python3 inference.py \\\n",
        "  --face {input_video_path} \\\n",
        "  --audio {input_audio_path} \\\n",
        "  --outfile results/output.mp4"
      ],
      "metadata": {
        "id": "D7hUwRCyUYEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the output video:"
      ],
      "metadata": {
        "id": "JB5RbKc-njkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# visualize code from makeittalk\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os, sys, glob, cv2, subprocess, platform\n",
        "\n",
        "def read_video(vid_name):\n",
        "  video_stream = cv2.VideoCapture(vid_name)\n",
        "  fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
        "  full_frames = []\n",
        "  while True:\n",
        "    still_reading, frame = video_stream.read()\n",
        "    if not still_reading:\n",
        "        video_stream.release()\n",
        "        break\n",
        "    full_frames.append(frame)\n",
        "  return full_frames, fps\n",
        "\n",
        "input_video_frames, fps = read_video(input_video_path)\n",
        "output_video_frames, _ = read_video('./results/output.mp4')\n",
        "\n",
        "frame_h, frame_w = input_video_frames[0].shape[:-1]\n",
        "out_concat = cv2.VideoWriter('./temp/temp/result_concat.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_w*2, frame_h))\n",
        "for i in range(len(output_video_frames)):\n",
        "  frame_input = input_video_frames[i % len(input_video_frames)]\n",
        "  frame_output = output_video_frames[i]\n",
        "  out_concat.write(cv2.hconcat([frame_input, frame_output]))\n",
        "out_concat.release()\n",
        "\n",
        "command = 'ffmpeg -loglevel error -y -i {} -i {} -strict -2 -q:v 1 {}'.format(input_audio_path, './temp/temp/result_concat.mp4', './results/output_concat_input.mp4')\n",
        "subprocess.call(command, shell=platform.system() != 'Windows')\n",
        "\n",
        "\n",
        "output_video_name = './results/output.mp4'\n",
        "output_video_mp4 = open('{}'.format(output_video_name),'rb').read()\n",
        "output_video_data_url = \"data:video/mp4;base64,\" + b64encode(output_video_mp4).decode()\n",
        "print('Display lip-syncing video: {}'.format(output_video_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <video height=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % output_video_data_url))\n",
        "\n",
        "output_concat_video_name = './results/output_concat_input.mp4'\n",
        "output_concat_video_mp4 = open('{}'.format(output_concat_video_name),'rb').read()\n",
        "output_concat_video_data_url = \"data:video/mp4;base64,\" + b64encode(output_concat_video_mp4).decode()\n",
        "print('Display input video and lip-syncing video: {}'.format(output_concat_video_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <video height=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % output_concat_video_data_url))\n"
      ],
      "metadata": {
        "id": "ravs9UDucMfy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}